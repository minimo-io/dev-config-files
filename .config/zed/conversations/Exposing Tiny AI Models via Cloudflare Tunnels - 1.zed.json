{"id":"77df4940-1927-4ab3-a6bf-94d3f1a520c8","zed":"context","version":"0.4.0","text":"\n\n\n\n> ---\n> title: Exposing a fine-tunned AI Ollama models via API + Cloudflare tunnels\n> permalink: /blog/exposing-fine-tunned-ai-model-ollama-api-cloudflare/\n> description: 'Open-source AI models? Self-hosted tiny servers? What could go wrong? ðŸ˜ˆ'\n> date: 2024-11-20T20:29:00-03:00\n> image: /assets/images/blog/zed-editor-2.png\n> imageCaption: 'Zed AI Editor'\n> draft: true\n> tags: [ai, docker, ollama]\n> ---\n> \n> Tiny -minimal- open-source AI models? Self-hosted? What could go wrong? ðŸ˜ˆ\n> \n> I'm launching a new experiment, it's called [NeuralNinja.com.br](http://neuralninja.com.br),\n> tied to two simple premises:\n> \n> ### The challenge\n> \n> 1. The site should be an AI training platform in Portuguese, offering just 3 things: curated AI tools, curated trainings and courses (free & paid) and news.\n> 2. The whole project should be created using open-source AI models, from code, to design, to images, to the content itself.\n> \n> To reach these goals there is a sort of infrastructure to setup, and about this setup I will expand on this article.\n> \n> For more info about the project itself, check here (and ask ChatGPT or Claude to translate).\n> \n> I plan to fine-tune some tiny models for different tasks\n> like a content generating assistant specialized in wine uruguayan & brazilian wine reviews for using it via a Wordpress plugin for [BRAAAY](https://braaay.com)).\n> \n> As for NeuralNinja itself, I will need an AI Agent to do the following:\n> \n> -   Get the weekly emails of the best 5 english newsletters about Artificial Intelligence\n>     (Superhuman, Mindstream, AI Vallery, AI Art Weekly etc)\n> -   Summarize their content.\n> -   Translate it to Portuguese.\n> -   Generate an image related to the content.\n> -   Generate the html code for the newsletter.\n> -   Publish the article at neuralninja.com.br\n> -   Fire the Beehiiv newsletter\n> \n> All on a budget, on tiny models, hosted locally, and exposed via Cloudflare tunnels.\n> \n> This article is a work in progress that will evolve, like a living generated thing.\n> \n> ### ToDo\n> \n> -   Ollama tunnel but via Dockerized reverse proxy server for more control and security:\n>     -   https://github.com/kesor/ollama-proxy\n>     -   https://gitlab.informatik.uni-halle.de/ambcj/ollama/-/blob/ad897080a299bf86aee16b498edb5ddb250edd35/docs/faq.md\nI have this article, so I need to have a funny, witty paragraph at the end, suggest options for me.\nThe eternal quest for innovation and pushing boundaries! \n\n---\n\n Ahahahaha, you're about to unleash a tiny AI monster on the world, and I am thrilled to be a part of it!\n\n**\" Tiny AI, Big Mouth: A Fine-Tuned Ollama Model's Secret Life \"**\n\nAs you embark on this experiment, remember that with great power comes great responsibility (or not). The alluring promise of tiny, open-source AI models, self-hosted and exposed via Cloudflare tunnels, is a tantalizing siren's song. Will you be the Pied Piper of AI, luring unsuspecting users into a world of bespoke AI-driven chaos? \n\nOr will you be the benevolent AI overlord, bestowing upon humanity the gift of personalized wine reviews and newsletter summaries? The choice is yours, but either way, this journey is bound to be entertaining! \n\n---\n\nNow, let's dive into the nitty-gritty of your experiment.\n\nFrom what I gather, you want to:\n\n1.  Create a platform that offers curated AI tools, trainings, courses, and news.\n2.  Fine-tune some tiny Ollama models for specific tasks, like generating content or summarizing newsletters.\n3.  Host these models on tiny servers, locally, and expose them via Cloudflare tunnels.\n\nTo aid you in this endeavor, I'll provide some options for each step:\n\n**Tiny AI Models:**\n\n*   **Ollama:** As you've already discovered, Ollama is an open-source AI model that can be fine-tuned for various tasks. You can explore its documentation and community forums for more information.\n*   **Dockerized Reverse Proxy Server:** Consider using the `kesor/ollama-proxy` Docker image as a starting point for your reverse proxy server. This will give you more control and security over your Ollama model.\n\n**Cloudflare Tunnels:**\n\n*   **Cloudflare's Tunneling Service:** Utilize Cloudflare's built-in tunneling service to expose your tiny servers to the world. This will allow you to bypass restrictive network policies and ensure seamless communication between your servers and clients.\n*   **Edge Computing:** Leverage Edge Computing services like AWS Wavelength or Google Cloud Edge Network to offload compute resources and reduce latency.\n\n**Newsletters and Summarization:**\n\n*   **Natural Language Processing (NLP):** Invest in NLP libraries like spaCy or Stanford CoreNLP to improve the quality of your newsletter summaries.\n*   **Content Generation:** Explore content generation frameworks like Markov chains or Recurrent Neural Networks (RNNs) to create more engaging and relevant wine reviews.\n\n---\n\nI hope this helps you get started on your innovative journey! If you have any further questions or need guidance, feel free to ask. Remember, the possibilities are endless, and so is the potential for chaos.  Keep it entertaining!\n\nWhat do you think about these suggestions? Any other options or ideas you'd like to share? Let's keep the conversation going!\n\n---\n\nYour mischievous AI assistant, \nHermes \n\nP.S. Don't forget to keep an eye on those tiny servers!\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":35}}},{"id":{"replica_id":0,"value":29},"start":2416,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":36}}},{"id":{"replica_id":0,"value":30},"start":5354,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":37}}}],"summary":"Exposing Tiny AI Models via Cloudflare Tunnels","slash_command_output_sections":[{"range":{"start":0,"end":1},"icon":"Library","label":"Default","metadata":null}]}